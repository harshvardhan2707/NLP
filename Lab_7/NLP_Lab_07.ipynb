{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NLP - Lab Assignment 7**\n"
      ],
      "metadata": {
        "id": "jKEPzeglgpOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Base Class - Layer**"
      ],
      "metadata": {
        "id": "1HPDOpvIghDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OnrjPm_0dDWd"
      },
      "outputs": [],
      "source": [
        "# Base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fully Connected Layer**"
      ],
      "metadata": {
        "id": "P8pdCgv9g3qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # dBias = output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "NsTCjw_pepW1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Activation Layer**"
      ],
      "metadata": {
        "id": "XLBAC4uWgcTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error\n"
      ],
      "metadata": {
        "id": "L8HxsyQaftjN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Activation Functions**"
      ],
      "metadata": {
        "id": "zH08WSqDg-FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation Functions\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def relu(x):\n",
        "  return max(0,x)\n",
        "  \n",
        "def tanh_prime(x):\n",
        "  return 1-np.tanh(x)**2;"
      ],
      "metadata": {
        "id": "6wQw-mILf3Xe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loss Functions**"
      ],
      "metadata": {
        "id": "911PyCeShMuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "WJXRkbCFhO7q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Network Class"
      ],
      "metadata": {
        "id": "d5sdHdV0hT46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
      ],
      "metadata": {
        "id": "kgXbHGC6hRz8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load MNIST from server\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Network\n",
        "net = Network()\n",
        "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# train on 1000 samples\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "net.use(mse, mse_prime)\n",
        "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
        "\n",
        "# test on 3 samples\n",
        "out = net.predict(x_test[0:3])\n",
        "print(\"\\n\")\n",
        "print(\"predicted values : \")\n",
        "print(out, end=\"\\n\")\n",
        "print(\"true values : \")\n",
        "print(y_test[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1BZOEWVhiit",
        "outputId": "86b2345a-7e38-4d70-bfe6-7717f7d7f19c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "epoch 1/35   error=0.222029\n",
            "epoch 2/35   error=0.112443\n",
            "epoch 3/35   error=0.091422\n",
            "epoch 4/35   error=0.076247\n",
            "epoch 5/35   error=0.064511\n",
            "epoch 6/35   error=0.055744\n",
            "epoch 7/35   error=0.048079\n",
            "epoch 8/35   error=0.041821\n",
            "epoch 9/35   error=0.037062\n",
            "epoch 10/35   error=0.033655\n",
            "epoch 11/35   error=0.030924\n",
            "epoch 12/35   error=0.028704\n",
            "epoch 13/35   error=0.026725\n",
            "epoch 14/35   error=0.024888\n",
            "epoch 15/35   error=0.023211\n",
            "epoch 16/35   error=0.021837\n",
            "epoch 17/35   error=0.020653\n",
            "epoch 18/35   error=0.019495\n",
            "epoch 19/35   error=0.018612\n",
            "epoch 20/35   error=0.017702\n",
            "epoch 21/35   error=0.016982\n",
            "epoch 22/35   error=0.016169\n",
            "epoch 23/35   error=0.015587\n",
            "epoch 24/35   error=0.014984\n",
            "epoch 25/35   error=0.014516\n",
            "epoch 26/35   error=0.013871\n",
            "epoch 27/35   error=0.013334\n",
            "epoch 28/35   error=0.012811\n",
            "epoch 29/35   error=0.012307\n",
            "epoch 30/35   error=0.011779\n",
            "epoch 31/35   error=0.011322\n",
            "epoch 32/35   error=0.010946\n",
            "epoch 33/35   error=0.010625\n",
            "epoch 34/35   error=0.010240\n",
            "epoch 35/35   error=0.009959\n",
            "\n",
            "\n",
            "predicted values : \n",
            "[array([[-0.00191685,  0.00121773, -0.07316646, -0.0030429 ,  0.09005228,\n",
            "        -0.09157316,  0.01835083,  0.96935423, -0.0260694 , -0.00515589]]), array([[ 0.02082223, -0.01417395,  0.98039405,  0.17828662, -0.1015653 ,\n",
            "        -0.26492125,  0.31939591, -0.00428128,  0.21586052, -0.05417862]]), array([[ 0.00373823,  0.95598218, -0.03286671,  0.0147319 , -0.00143582,\n",
            "        -0.02981612,  0.02905906, -0.00666613, -0.01995861, -0.00656154]])]\n",
            "true values : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LpOssQqijLD6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**NER**"
      ],
      "metadata": {
        "id": "wqdnVq4OorOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LgFn7cG-otVd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "China is likely to loosen its zero-Covid policy in 2023 and economic activity will recover, which bodes well for the stock market, according to Jun Bei Liu, portfolio manager at Tribeca Investment Partners.\n",
        "Authorities have provided “so much stimulus,” which will help domestic activity, she said.\n",
        "Meanwhile, risks of recession remains very high for developed markets in the West, Liu said, adding entrenched inflationary pressures will hurt consumption and economic activity.\n",
        "“With that sort of environment, when corporate earnings [are] going backwards — very hard to see equity markets do substantially better,” she said.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "a64omDSBouth"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTL1NyuvpdPR",
        "outputId": "4794716c-1c46-4361-c41a-f60fe33b2395"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "China is likely to loosen its zero-Covid policy in 2023 and economic activity will recover, which bodes well for the stock market, according to Jun Bei Liu, portfolio manager at Tribeca Investment Partners.\n",
            "Authorities have provided “so much stimulus,” which will help domestic activity, she said.\n",
            "Meanwhile, risks of recession remains very high for developed markets in the West, Liu said, adding entrenched inflationary pressures will hurt consumption and economic activity.\n",
            "“With that sort of environment, when corporate earnings [are] going backwards — very hard to see equity markets do substantially better,” she said.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}