{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Harshvardhan Aditya- E20CSE365"
      ],
      "metadata": {
        "id": "mqnfgPuMUeMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pur6RnuFQkqq",
        "outputId": "ac3c3c2f-3080-4e0c-8f4a-d7672a8035ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n",
            "NORM FORM\n",
            "   (0, 1)\t0.4697913855799205\n",
            "  (0, 2)\t0.580285823684436\n",
            "  (0, 3)\t0.3840852409148149\n",
            "  (0, 6)\t0.3840852409148149\n",
            "  (0, 8)\t0.3840852409148149\n",
            "  (1, 1)\t0.6876235979836937\n",
            "  (1, 3)\t0.2810886740337529\n",
            "  (1, 5)\t0.5386476208856762\n",
            "  (1, 6)\t0.2810886740337529\n",
            "  (1, 8)\t0.2810886740337529\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (3, 1)\t0.4697913855799205\n",
            "  (3, 2)\t0.580285823684436\n",
            "  (3, 3)\t0.3840852409148149\n",
            "  (3, 6)\t0.3840852409148149\n",
            "  (3, 8)\t0.3840852409148149\n",
            "(4, 9)\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "corpus = [\n",
        "      'this is the first document',\n",
        "      'this document is the second document',\n",
        "      'and this is the third one',\n",
        "      'is this the first document',\n",
        " ] \n",
        "\n",
        "def IDF(corpus, unique_words):\n",
        "   idf_dict={}\n",
        "   N=len(corpus)\n",
        "   for i in unique_words:\n",
        "     count=0\n",
        "     for sen in corpus:\n",
        "       if i in sen.split():\n",
        "         count=count+1\n",
        "       idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
        "   return idf_dict \n",
        "\n",
        "def fit(whole_data):\n",
        "    unique_words = set()\n",
        "    if isinstance(whole_data, (list,)):\n",
        "      for x in whole_data:\n",
        "        for y in x.split():\n",
        "          if len(y)<2:\n",
        "            continue\n",
        "          unique_words.add(y)\n",
        "      unique_words = sorted(list(unique_words))\n",
        "      vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "      Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
        "    return vocab, Idf_values_of_all_unique_words\n",
        "Vocabulary,idf_of_vocabulary=fit(corpus)\n",
        "print(list(Vocabulary.keys())) \n",
        "print(list(idf_of_vocabulary.values()))\n",
        "def transform(dataset,vocabulary,idf_values):\n",
        "     sparse_matrix= csr_matrix( (len(dataset), len(vocabulary)), dtype=np.float64)\n",
        "     for row  in range(0,len(dataset)):\n",
        "       number_of_words_in_sentence=Counter(dataset[row].split())\n",
        "       for word in dataset[row].split():\n",
        "           if word in  list(vocabulary.keys()):\n",
        "               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
        "               sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
        "     print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
        "     output =normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
        "     return output\n",
        "final_output=transform(corpus,Vocabulary,idf_of_vocabulary)\n",
        "print(final_output.shape)\n",
        "print(final_output[0].toarray())\n",
        " \n"
      ]
    }
  ]
}