{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Harshvardhan Aditya- E20CSE365"
      ],
      "metadata": {
        "id": "MiqwryCySDmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count vectorizer"
      ],
      "metadata": {
        "id": "5OdnEf-_SIMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svjsE6pzbKOy",
        "outputId": "b2eb99bc-dd63-4a1a-ef4e-2e4b6066c42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:  {'one': 9, 'geek': 3, 'helps': 7, 'two': 11, 'geeks': 4, 'help': 6, 'four': 2, 'each': 1, 'many': 8, 'other': 10, 'at': 0, 'geeksforgeeks': 5}\n",
            "Encoded Document is:\n",
            "[[0 0 0 1 1 0 0 1 0 1 0 1]\n",
            " [0 0 1 0 2 0 1 0 0 0 0 1]\n",
            " [1 1 0 1 1 1 0 1 1 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# Count Vectorization\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        " \n",
        "document = [\"One Geek helps Two Geeks\",\n",
        "            \"Two Geeks help Four Geeks\",\n",
        "            \"Each Geek helps many other Geeks at GeeksforGeeks\"]\n",
        " \n",
        "# Create a Vectorizer Object\n",
        "vectorizer = CountVectorizer()\n",
        " \n",
        "vectorizer.fit(document)\n",
        " \n",
        "# Printing the identified Unique words along with their indices\n",
        "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
        " \n",
        "# Encode the Document\n",
        "vector = vectorizer.transform(document)\n",
        " \n",
        "# Summarizing the Encoded Texts\n",
        "print(\"Encoded Document is:\")\n",
        "print(vector.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding\n",
        "\n",
        "import numpy as np\n",
        "### Categorical data to be converted to numeric data\n",
        "colors = [\"red\", \"green\", \"yellow\", \"red\", \"blue\"]\n",
        "\n",
        "### Universal list of colors\n",
        "total_colors = [\"red\", \"green\", \"blue\", \"black\", \"yellow\",\"indigo\"]\n",
        "\n",
        "### map each color to an integer\n",
        "mapping = {}\n",
        "for x in range(len(total_colors)):\n",
        "  mapping[total_colors[x]] = x\n",
        "\n",
        "one_hot_encode = []\n",
        "\n",
        "for c in colors:\n",
        "  arr = list(np.zeros(len(total_colors), dtype = int))\n",
        "  arr[mapping[c]] = 1\n",
        "  one_hot_encode.append(arr)\n",
        "\n",
        "print(one_hot_encode)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH6nSsE3mCxx",
        "outputId": "251f5bdf-0ccd-432c-fc00-8cc57d057ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "### Categorical data to be converted to numeric data\n",
        "colors = ([\"red\", \"green\", \"yellow\", \"red\", \"blue\"])\n",
        "\n",
        "### integer mapping using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(colors)\n",
        "print(integer_encoded)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "\n",
        "### One hot encoding\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "print(onehot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU8zCwQc50AK",
        "outputId": "587e2d14-328a-4e4b-a81a-2b0012a1eb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 1 3 2 0]\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words"
      ],
      "metadata": {
        "id": "83VDJ_F1k0JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens):\n",
        "    ''' This function takes list of words in a sentence as input \n",
        "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
        "    word is not present in tokens and count of token if present.'''\n",
        "    vector=[]\n",
        "    for w in filtered_vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector\n",
        "def unique(sequence):\n",
        "    '''This functions returns a list in which the order remains \n",
        "    same and no item repeats.Using the set() function does not \n",
        "    preserve the original ordering,so i didnt use that instead'''\n",
        "    seen = set()\n",
        "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
        "#create a list of stopwords.You can import stopwords from nltk too\n",
        "stopwords=[\"to\",\"is\",\"a\"]\n",
        "#list of special characters.You can use regular expressions too\n",
        "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
        "#Write the sentences in the corpus,in our case, just two \n",
        "string1=\"Welcome to Great Learning , Now start learning\"\n",
        "string2=\"Learning is a good practice\"\n",
        "#convert them to lower case\n",
        "string1=string1.lower()\n",
        "string2=string2.lower()\n",
        "#split the sentences into tokens\n",
        "tokens1=string1.split()\n",
        "tokens2=string2.split()\n",
        "print(tokens1)\n",
        "print(tokens2)\n",
        "#create a vocabulary list\n",
        "vocab=unique(tokens1+tokens2)\n",
        "print(vocab)\n",
        "#filter the vocabulary list\n",
        "filtered_vocab=[]\n",
        "for w in vocab: \n",
        "    if w not in stopwords and w not in special_char: \n",
        "        filtered_vocab.append(w)\n",
        "print(filtered_vocab)\n",
        "#convert sentences into vectords\n",
        "vector1=vectorize(tokens1)\n",
        "print(vector1)\n",
        "vector2=vectorize(tokens2)\n",
        "print(vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dRU9xz0FYC0",
        "outputId": "8f25b7f0-0bee-48a9-ba13-971167258e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
            "['learning', 'is', 'a', 'good', 'practice']\n",
            "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
            "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
            "[1, 1, 2, 1, 1, 0, 0]\n",
            "[0, 0, 1, 0, 0, 1, 1]\n"
          ]
        }
      ]
    }
  ]
}