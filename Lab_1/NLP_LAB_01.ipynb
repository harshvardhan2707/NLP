{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7j7iAV-rEiV",
        "outputId": "c2299c8e-faa4-499e-a7b7-a4f26ffbacaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rG0P_6e9mdtP"
      },
      "outputs": [],
      "source": [
        "sentence=\"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multiplanet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately\n",
        "developed liquid-fuel launch vehicle to orbit the Earth.\"\"\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBnWHEuCrVXQ",
        "outputId": "968458e3-bfc5-4533-9976-10c274d43863"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’',\n",
              " 's',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multiplanet',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self-sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars',\n",
              " '.',\n",
              " 'In',\n",
              " '2008',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’',\n",
              " 's',\n",
              " 'Falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " 'liquid-fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(sentence)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOP1_IYSrdUS",
        "outputId": "4165570e-9c0f-4e05-d357-ddf2b3fb0357"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['found',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'spacex',\n",
              " '’',\n",
              " 's',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enabl',\n",
              " 'human',\n",
              " 'to',\n",
              " 'becom',\n",
              " 'a',\n",
              " 'spacefar',\n",
              " 'civil',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multiplanet',\n",
              " 'speci',\n",
              " 'by',\n",
              " 'build',\n",
              " 'a',\n",
              " 'self-sustain',\n",
              " 'citi',\n",
              " 'on',\n",
              " 'mar',\n",
              " '.',\n",
              " 'in',\n",
              " '2008',\n",
              " ',',\n",
              " 'spacex',\n",
              " '’',\n",
              " 's',\n",
              " 'falcon',\n",
              " '1',\n",
              " 'becam',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privat',\n",
              " 'develop',\n",
              " 'liquid-fuel',\n",
              " 'launch',\n",
              " 'vehicl',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "ps = PorterStemmer()\n",
        "Stemmed=[ps.stem(i) for i in tokens]\n",
        "Stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6SPKY1qtPnr",
        "outputId": "1d1362e8-9f81-4134-b848-db251dcb467b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['found',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'spacex',\n",
              " '’',\n",
              " 's',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enabl',\n",
              " 'human',\n",
              " 'to',\n",
              " 'becom',\n",
              " 'a',\n",
              " 'spacefar',\n",
              " 'civil',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multiplanet',\n",
              " 'speci',\n",
              " 'by',\n",
              " 'build',\n",
              " 'a',\n",
              " 'self-sustain',\n",
              " 'citi',\n",
              " 'on',\n",
              " 'mar',\n",
              " '.',\n",
              " 'in',\n",
              " '2008',\n",
              " ',',\n",
              " 'spacex',\n",
              " '’',\n",
              " 's',\n",
              " 'falcon',\n",
              " '1',\n",
              " 'becam',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privat',\n",
              " 'develop',\n",
              " 'liquid-fuel',\n",
              " 'launch',\n",
              " 'vehicl',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "Lemmatized=[lemmatizer.lemmatize(i) for i in Stemmed]\n",
        "Lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "s1 = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "word_tokens = word_tokenize(s1)\n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  \n",
        "filtered_sentence = []\n",
        "stop_words_in_sentence=[]\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "    else:\n",
        "      stop_words_in_sentence.append(w)\n",
        "  \n",
        "print(\"Tokenized words\",word_tokens)\n",
        "print(\"Filtered sentence\",filtered_sentence)\n",
        "print(\"Stop words in sentence\",stop_words_in_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1LwJrSPwN1O",
        "outputId": "35b01188-bca6-488f-955f-b5db2d7e625d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "Filtered sentence ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
            "Stop words in sentence ['is', 'a', 'off', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s2=\"\"\"Nick likes to play footbNick likes to play football, however he is not too fond of tennis.all, however he\n",
        "is not too fond of tennis.\"\"\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "word_tokens = word_tokenize(s2)\n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  \n",
        "filtered_sentence = []\n",
        "stop_words_in_sentence=[]\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "    else:\n",
        "      stop_words_in_sentence.append(w)\n",
        "  \n",
        "print(\"Tokenized words\",word_tokens)\n",
        "print(\"Filtered sentence\",filtered_sentence)\n",
        "print(\"Stop words in sentence\",stop_words_in_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK-Tj5dZz4g4",
        "outputId": "e3faff34-f292-49db-ffe1-27cb38f2e60d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words ['Nick', 'likes', 'to', 'play', 'footbNick', 'likes', 'to', 'play', 'football', ',', 'however', 'he', 'is', 'not', 'too', 'fond', 'of', 'tennis.all', ',', 'however', 'he', 'is', 'not', 'too', 'fond', 'of', 'tennis', '.']\n",
            "Filtered sentence ['Nick', 'likes', 'play', 'footbNick', 'likes', 'play', 'football', ',', 'however', 'fond', 'tennis.all', ',', 'however', 'fond', 'tennis', '.']\n",
            "Stop words in sentence ['to', 'to', 'he', 'is', 'not', 'too', 'of', 'he', 'is', 'not', 'too', 'of']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}